# -*- coding: utf-8 -*-
"""DSPSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGZN1leaX7mAC1lM5mLzA9LSezOsi3xy
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import pdb

class ARXRegressor:
    def __init__(self, na, nb, nk):
        # [na nb nk] denote the regressor structure (past input and outputs) for the time series model
        self.na = [na] if isinstance(na, int) else na  # Ensure list format for na
        self.nb = [nb] if isinstance(nb, int) else nb
        self.nk = [nk] if isinstance(nk, int) else nk
        self.model = LinearRegression()

    def create_reg(self, y, u):
        # creates the vector of output and the regressor matrix

        y = np.atleast_2d(y).T if y.ndim == 1 else np.array(y)
        u = np.atleast_2d(u).T if u.ndim == 1 else np.array(u)

        n_samples, n_outputs = y.shape
        _, n_inputs = u.shape
        # pdb.set_trace()
        # Ensure consistent dimensions for MIMO and SISO cases
        if len(self.na) != n_outputs:
            self.na = np.array( [self.na[0]] * n_outputs, dtype=np.int32 )
        if len(self.nb) != n_inputs:
            self.nb = np.array( [self.nb[0]] * n_inputs, dtype=np.int32 )
        if len(self.nk) != n_inputs:
            self.nk = np.array( [self.nk[0]] * n_inputs, dtype=np.int32 )

        # Calculate the required starting index
        # pdb.set_trace()
        if len([nb_iter + nk_iter for nb_iter, nk_iter in zip(self.nb[0], self.nk[0])])== 0:
            start_idx = int(np.max(self.na))
        else:
            start_idx = int( max(np.max(self.na), np.max( [nb + nk for nb, nk in zip(self.nb[0], self.nk[0])] ) ) )

        X = []
        Y = []
        # pdb.set_trace()

        for t in range(start_idx, n_samples):
            reg = []

            # Collect past outputs
            for j in range(n_outputs):
                for lag in range(self.na.flatten()[j] ):
                    reg.extend(np.array([-y[t - lag - 1, j]]).flatten() )

            # Collect past inputs with delay
            for k in range(n_inputs):
                for lag in range(self.nb.flatten()[k]):
                    reg.extend(np.array(u[t - self.nk[k] - lag, k]).flatten())

            # if t<5:
            #     print(np.array(reg).flatten())
            X.append(reg)
            Y.append(y[t, :])

        # print(X[:3])
        return np.array(X), np.array(Y)

    def train(self, y, u):
        #calls the create reg function and creates the model
        X, Y = self.create_reg(y, u)
        self.model.fit(X, Y)

    def predict(self, y, u):
        # returns the predicted output
        X, _ = self.create_reg(y, u)
        return self.model.predict(X)

    def fit(self, y, u):
        #Function to generates the fit percentage
        y = np.atleast_2d(y).T if y.ndim == 1 else np.array(y)
        u = np.atleast_2d(u).T if u.ndim == 1 else np.array(u)

        # Predict outputs
        yhat = self.predict(y, u)

        # Align data for comparison
        trunc_val = len(yhat)
        ytrue = y[-trunc_val:, :]

        # Calculate NRMSE for each output
        nrmse_list = []
        for i in range(ytrue.shape[1]):
            error_norm = np.linalg.norm(ytrue[:, i] - yhat[:, i])
            mean_norm = np.linalg.norm(ytrue[:, i] - np.mean(ytrue[:, i]))

            if mean_norm != 0:
                nrmse = (1 - error_norm / mean_norm) * 100
            else:
                nrmse = 0.0  # Avoid division by zero if all values are constant

            nrmse_list.append(nrmse)

        return nrmse_list

# # Unit Testing the ARXRegressor class
# if __name__ == "__main__":
#     # Simulated input-output dataset
#     data_size = 100
#     np.random.seed(42)
#     u = np.random.randn(data_size)  # Input signal
#     y = np.zeros(data_size)  # Output signal

#     # Generate a simple simulated output
#     for t in range(3, data_size):
#         y[t] = 0.5 * y[t - 1] - 0.3 * y[t - 2] + 0.8 * u[t - 1] + 0.6 * u[t - 2] + np.random.randn() * 0.1

#     # Define ARX model structure [na, nb, nk]
#     na, nb, nk = 2, 2, 1

#     # Train ARX model
#     arx = ARXRegressor(na, nb, nk)
#     arx.train(y, u)

#     # Make predictions
#     y_pred = arx.predict(y,u)
#     fit_per = arx.fit(y, u)

#     # Compare actual and predicted values
#     import matplotlib.pyplot as plt

#     plt.plot(y, label="Actual Output")
#     plt.plot(range(max(na, nb + nk), len(y)), y_pred, label="Predicted Output", linestyle="--")
#     plt.legend()
#     plt.xlabel("Time Step")
#     plt.ylabel("Output")
#     plt.title(f"ARX Model Prediction. NRMSE = {round(fit_per,2)} %")
#     plt.show()

### Create Dataset.

# ARX parameters
d_na = 2  # Number of past output terms
d_nb = 2  # Number of past input terms
d_nk = 1  # Input delay
n_inputs = 5  # Number of inputs
n_samples = 100  # Number of samples in the dataset

# Generate random coefficients for ARX model
a_coeffs = np.array([-0.6, 0.2])  # Example coefficients for na terms
b_coeffs = np.random.uniform(-1, 1, (n_inputs, d_nb))  # Random coefficients for nb terms

# Generate random input signals
inputs = np.random.rand(n_samples, n_inputs)  # Shape (n_samples, n_inputs)

# Initialize output and noise
output = np.zeros(n_samples)  # Output signal
noise = 0.01 * np.random.randn(n_samples)  # Small random noise

# Simulate the ARX process
for t in range(max(d_na, d_nb + d_nk), n_samples):
    # Contribution from past outputs
    for i in range(d_na):
        output[t]+= -a_coeffs[i] * output[t - i - 1]

    # Contribution from inputs (delayed by nk)
    for j in range(n_inputs):
        for k in range(d_nb):
            # Fix: Ensure correct indexing and scalar multiplication
            output[t] += b_coeffs[j, k] * inputs[t - d_nk - k, j]

    # Add noise
    output[t] += noise[t]

# Create the dataset

# print(output.shape, inputs.shape)
dataset = {
    "inputs": inputs,
    "output": output.reshape((len(output),1))
}

# CSV FILE: matrix N,6, 1st col output
iodata= np.hstack([dataset["output"], dataset["inputs"]])
# print(iodata.shape)
# iodata = np.random.rand(100,6)
est_len = 65
overallest = iodata[:est_len]
overallval = iodata[est_len:]

# USER INPUTS
Niterations= 50
spsaw= 1
estw, valw, ovew = 0, 1.0, 0
weights= [estw, valw, ovew ]

# Feature selection params
ucol= 5

# Create binary vector w of length of inputs
w = 0.5 * np.ones((ucol, 1))  # Initialize input vector

# Initialize parameters
ck = 0.1  # Small perturbation size
afs = 0.003  # a > 0
Afs = 0.1 * Niterations  # A >= 0
alphafs = 0.501  # 0.5 < alpha <= 1
gainfs = [afs, Afs, alphafs, ck]  # Save ak and ck values

# Parameters for ARX order optimization
carx = 1  # No impact on calculations if carx = 1 (as per Wang & Spall 2014)
aarx = 0.1  # a > 0, recommended range: 0.1 to 0.3
Aarx = 0.1 * Niterations  # A >= 0
alphaarx = 0.501  # 0.5 < alpha <= 1
gainarx = [aarx, Aarx, alphaarx, carx]  # Save ak and ck values for ARX

# ARX structure initialization
arxstructure = [2, 2, 1]  # Change structure as needed
ny = 1  # Assuming number of outputs is 1
nu = 5 # Assuming number of inputs is 5

# Initialize ARX values
na = arxstructure[0] * np.ones((ny, ny), dtype=int)
nb = arxstructure[1] * np.ones((ny, nu), dtype=int)
nk = arxstructure[2] * np.ones((ny, nu), dtype=int)

# Specify constant ARX orders
naconst = 0
NA = 1

nbconst = 0
NB = 1

nkconst = 0
NK = 1

# Specify ARX order bounds
naub = 3
nalb = 1

nbub = 3
nblb = 1

nkub = 2
nklb = 1


arxmodel_f_vec = []
loss_valf_vec = []
loss_ovrf_vec = []
narxs_f_vec = []
wf_vec = []

# SPSA Iterations
for k in range(1, Niterations + 1):
    ak = afs / (k + Afs + 1) ** alphafs
    akarx = aarx / (k + Aarx + 1) ** alphaarx
    c = ck

    #### Initialize Feature Selection
    if spsaw == 1:  # If using SPSA for feature selection
        # New values of w
        wint = np.floor(w) + 0.5  # Step 3 of Wang & Spall 2014

        wplus = np.zeros((ucol, 1))  # Initialize wplus and wminus
        wminus = np.zeros((ucol, 1))
        z = np.zeros((ucol, 1))  # To prevent all zeros in wplus or wminus

        while True:  # Loop to ensure no all-zero wplus or wminus
            delta = np.zeros((len(w), 1))  # Random perturbation for each input
            for i in range(len(w)):
                delta[i, 0] = 2 * np.round(np.random.rand()) - 1

                # Create two points to estimate the gradient
                wplus[i, 0] = wint[i, 0] + ck * delta[i, 0]
                wminus[i, 0] = wint[i, 0] - ck * delta[i, 0]

            # Check if wplus or wminus is all zeros
            if not (np.array_equal(np.round(wplus), z) or np.array_equal(np.round(wminus), z)):
                break

        # Set bounds on wplus and wminus (binary constraints)
        wplus = np.clip(wplus, 0, 1)
        wminus = np.clip(wminus, 0, 1)

    else:  # If SPSA is NOT used for feature selection
        w = np.ones((ucol, 1))
        wplus = w
        wminus = w
        # delta = np.ones((ucol, 1))

    #### Initialize Regressor Orders

    b = ucol

    arxzerosplus = np.where(np.round(wplus.T) == 0)[1]
    arxzerosminus = np.where(np.round(wminus.T) == 0)[1]

    if naconst == 1:
        # If naconst == 1, set naplus and naminus to constant values
        naplus = NA * np.ones_like(na)
        naminus = NA * np.ones_like(na)
    else:
        napm = np.floor(na) + 0.5  # Recommended from Step 3 of Wang & Spall 2014
        naplus = np.zeros_like(na)
        naminus = np.zeros_like(na)
        deltana = np.zeros((1, ny))  # Initialize deltana as a row vector

        for i in range(ny):  # Loop over the number of outputs
            deltana[0, i] = 2 * np.round(np.random.rand()) - 1  # Random perturbation for each input

            # Compute naplus
            naplus[0, i] = napm[0, i] + carx * deltana[0, i]
            if naplus[0, i] < nalb:  # Bound ARX orders
                naplus[0, i] = nalb
            if naplus[0, i] > naub:
                naplus[0, i] = naub

            # Compute naminus
            naminus[0, i] = napm[0, i] - carx * deltana[0, i]
            if naminus[0, i] < nalb:  # Bound ARX orders
                naminus[0, i] = nalb
            if naminus[0, i] > naub:  # Bound ARX orders
                naminus[0, i] = naub

    if nbconst == 1:
        # If nbconst == 1, set nbplus and nbminus to constant values
        nbplus = NB * np.ones_like(nb)
        nbminus = NB * np.ones_like(nb)
    else:
        nbpm = np.floor(nb) + 0.5
        nbplus = np.zeros_like(nb)
        nbminus = np.zeros_like(nb)
        deltanb = np.zeros((1, b))
        deltank = np.zeros((1, b))

        for i in range(b):  # Loop over the number of inputs (assumes ny = 1)
            # Generate random perturbations
            deltanb[0, i] = 2 * np.round(np.random.rand()) - 1
            deltank[0, i] = 2 * np.round(np.random.rand()) - 1

            nbplus[0, i] = nbpm[0, i] + carx * deltanb[0, i]
            if nbplus[0, i] < nblb:
                nbplus[0, i] = nblb
            if nbplus[0, i] > nbub:
                nbplus[0, i] = nbub

            # Compute nbminus and bound ARX orders
            nbminus[0, i] = nbpm[0, i] - carx * deltanb[0, i]
            if nbminus[0, i] < nblb:
                nbminus[0, i] = nblb
            if nbminus[0, i] > nbub:
                nbminus[0, i] = nbub


    if nkconst == 1:
        # If nkconst == 1, set nkplus and nkminus to constant values
        nkplus = NK * np.ones_like(nk)
        nkminus = NK * np.ones_like(nk)
    else:
        nkpm = np.floor(nk) + 0.5
        nkplus = np.zeros_like(nk)
        nkminus = np.zeros_like(nk)

        for i in range(b):

            nkplus[0, i] = nkpm[0, i] + carx * deltank[0, i]
            if nkplus[0, i] < nklb:            nkplus[0, i] = nklb
            if nkplus[0, i] > nkub:  # Upper bound
                nkplus[0, i] = nkub
            if nkplus[0, i] == 0 and nbplus[0, i] == 0:  # Prevent nb and nk both being 0
                nkplus[0, i] = nkub  # Assuming nkub = 1

            # Compute nkminus
            nkminus[0, i] = nkpm[0, i] - carx * deltank[0, i]
            if nkminus[0, i] < nklb:
                nkminus[0, i] = nklb
            if nkminus[0, i] > nkub:
                nkminus[0, i] = nkub
            if nkminus[0, i] == 0 and nbminus[0, i] == 0:
                nkminus[0, i] = nkub

    # Resize nbplus and nbminus by removing columns corresponding to arxzerosplus and arxzerosminus
    nbplus = np.delete(nbplus, arxzerosplus, axis=1)
    nbminus = np.delete(nbminus, arxzerosminus, axis=1)

    # Similar operation for nkplus and nkminus
    nkplus = np.delete(nkplus, arxzerosplus, axis=1)
    nkminus = np.delete(nkminus, arxzerosminus, axis=1)

### --------- RUN ON DATA IODATA ---------

    # PLUS

    wplus_inp = np.array( np.where(np.round(wplus) == 1) ) # Find indices where wplus is 1
    # print(f"wplus= {wplus_inp}\twplus={wplus}")
    ovrplus_u = iodata[:, 1:][:,wplus_inp[0]]
    estplus_u = overallest[:, 1:][:,wplus_inp[0]]
    valplus_u = overallval[:, 1:][:,wplus_inp[0]]

    nyplus, nuplus = wplus_inp.shape
    narxsplus = np.round(np.hstack([
        naplus * np.ones((ny, ny)),
        nbplus * np.ones((ny, nuplus)),
        nkplus * np.ones((ny, nuplus))
    ]))
    stplus = 1

    # MINUS
    wminus_inp = np.array( np.where(np.round(wminus) == 1) )  # Find indices where wminus is 1
    ovrminus_u = iodata[:, 1:][:,wminus_inp[0]]
    estminus_u = overallest[:, 1:][:,wminus_inp[0]]
    valminus_u = overallval[:, 1:][:,wminus_inp[0]]

    nyminus, numinus = wminus_inp.shape
    narxsminus = np.round(np.hstack([
        naminus * np.ones((ny, ny)),
        nbminus * np.ones((ny, numinus)),
        nkminus * np.ones((ny, numinus))
    ]))
    stminus = 1


    # print("NA:",na,nb,nk)
    # Mean Subtracted Validation Data
    for _ in range(ny):  # Loop over outputs
        arxmodelplus = ARXRegressor(naplus,nbplus,nkplus)
        arxmodelplus.train(overallest[:,0], estplus_u )

        loss_valplus= arxmodelplus.fit(overallval[:,0], valplus_u)
        loss_ovrplus= arxmodelplus.fit(iodata[:,0], ovrplus_u)

        arxmodelminus = ARXRegressor(naminus,nbminus,nkminus)
        arxmodelminus.train(overallest[:,0], estminus_u)

        loss_valminus= arxmodelminus.fit(overallval[:,0], valminus_u)
        loss_ovrminus= arxmodelminus.fit(iodata[:,0], ovrminus_u)

        # print(loss_ovrplus, loss_ovrminus)

    # Use WEIGHTED average of all 3 percent fits
    percentfitarxplusavg = (valw * loss_valplus[0] + ovew * loss_ovrplus[0])
    percentfitarxminavg = (valw * loss_valminus[0] + ovew * loss_ovrminus[0] )

    # Approximate gradient(s)
    if spsaw == 1:  # Only update w if SPSA is used for feature selection
        gk = -1 * ((percentfitarxplusavg - percentfitarxminavg) / (2 * ck)) * delta
        w_update = ak * gk  # Tuning ak (w)
    else:
        gk = 0  # No update if feature selection is not used

    # Gradients for ARX parameters
    gkna = -1 * ((percentfitarxplusavg - percentfitarxminavg) / (2 * carx)) * (1 / deltana)
    na_update = gkna * akarx  # Tuning ak (arx)

    gknb = -1 * ((percentfitarxplusavg - percentfitarxminavg) / (2 * carx)) * (1 / deltanb)
    gknk = -1 * ((percentfitarxplusavg - percentfitarxminavg) / (2 * carx)) * (1 / deltank)

    # Update weights and ARX orders
    w = w - ak * gk
    na = na - akarx * gkna
    nb = nb - akarx * gknb
    nk = nk - akarx * gknk

    # Set bounds on values of w between 0 and 1
    w = np.clip(w, 0, 1)

    # Set bounds for ARX orders
    na = np.clip(na, nalb, naub)

    # For nb
    for i in range(len(nb[0])):
        if nb[0][i] < nblb:

            nb[0][i] = nblb

        if nb[0][i] > nbub:
            nb[0][i] = nbub

    # For nk
    for i in range(len(nk[0])):
        if nk[0][i] < nklb:
            nk[0][i] = nklb
        if nk[0][i] > nkub:
            nk[0][i] = nkub
        if round(nb[0][i]) == 0 and round(nk[0][i]) == 0:
            nk[0][i] = nkub

    # Handle constant held values for na, nb, nk
    if naconst == 1:
        na = NA * np.ones_like(na)

    if nbconst == 1:
        nb = NB * np.ones_like(nb)

    if nkconst == 1:
        nk = NK * np.ones_like(nk)

    na,nb,nk = np.round(na).astype(np.int32), np.round(nb).astype(np.int32), np.round(nk).astype(np.int32)

    # EVALUATE PERTURBED MODEL
    w = np.round(w)
    wfinal_inp = np.array(np.where(w == 1))

    est_fu = overallest[:,1:][:,wfinal_inp[0]]
    ovr_fu = iodata[:, 1:][:,wfinal_inp[0]]
    val_fu = overallval[:, 1:][:,wfinal_inp[0]]

    # pdb.set_trace()
    # nufinal = len(wfinal_inp[0])

    narxs_f = np.round(np.hstack([na,nb,nk]))

    arxmodel_f = ARXRegressor(na,nb,nk)
    arxmodel_f.train(overallest[:,0], est_fu)

    loss_valf= arxmodel_f.fit(overallval[:,0], val_fu)
    loss_ovrf= arxmodel_f.fit(iodata[:,0], ovr_fu)

    # print(loss_valf, loss_ovrf)
    arxmodel_f_vec.append(arxmodel_f)
    loss_valf_vec.append(loss_valf)
    loss_ovrf_vec.append(loss_ovrf)
    narxs_f_vec.append(narxs_f)
    wf_vec.append(w)

import matplotlib.pyplot as plt
for idx, vec in enumerate([loss_valf_vec, loss_ovrf_vec]):
    plt.figure(idx)
    plt.plot(vec, label="Validation Fit" if idx == 0 else "Overall Fit")
    plt.xlabel("Iterations")
    plt.ylabel("Fit Percentage (NRMSE)")
    plt.title("NRMSE Fit Percentages of DSPSA-generated models on datasets")
    plt.legend()

plt.show()

